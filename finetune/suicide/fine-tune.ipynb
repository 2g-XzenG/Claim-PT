{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "wicked-finder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 202 µs\n"
     ]
    }
   ],
   "source": [
    "%load_ext autotime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "random-fluid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "diverse-vegetation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========LOADING DATA==========\n",
      "time: 41.8 ms\n"
     ]
    }
   ],
   "source": [
    "print(\"==========LOADING DATA==========\")\n",
    "age_seq = pickle.load(open(\"../data/new_age_seq\",\"rb\"))\n",
    "sex_seq = pickle.load(open(\"../data/new_sex_seq\",\"rb\"))\n",
    "\n",
    "util_seq = pickle.load(open(\"../data/new_util_seq\",\"rb\"))\n",
    "code_seq = pickle.load(open(\"../data/new_code_seq\",\"rb\"))\n",
    "date_seq = pickle.load(open(\"../data/new_date_seq\",\"rb\"))\n",
    "label_seq = pickle.load(open(\"../data/new_label_seq\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "disabled-favor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.04 s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hollow-environment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 10 ms\n"
     ]
    }
   ],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, seqs, vocab_sizes, list_IDs, max_visit, max_code, batch_size=100, shuffle=True):\n",
    "        self.seqs = seqs\n",
    "        self.code_vocab = vocab_sizes[0]\n",
    "        self.cat_vocab = vocab_sizes[1]\n",
    "        self.list_IDs = list_IDs\n",
    "        self.max_visit = max_visit\n",
    "        self.max_code = max_code\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch' \n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' \n",
    "        demo_feature, code_feature, util_feature, date_feature, cls_feature = self.seqs\n",
    "        batch_demo, batch_code, batch_util, batch_date, batch_cls = [], [], [], [], []\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            batch_demo.append(demo_feature[ID])\n",
    "            batch_code.append(code_feature[ID])\n",
    "            batch_util.append(util_feature[ID])\n",
    "            batch_date.append(date_feature[ID])\n",
    "            batch_cls.append(cls_feature[ID])\n",
    "        \n",
    "        batch_demo_feature = np.array(batch_demo)\n",
    "        batch_code_feature = self.code_padding(batch_code)\n",
    "        batch_util_feature = self.date_padding(batch_util)\n",
    "        batch_date_feature = self.date_padding(batch_date)\n",
    "        batch_cls = np.array(batch_cls)\n",
    "        \n",
    "        dic = (\n",
    "            {\n",
    "                'demo_feature': batch_demo_feature,\n",
    "                'code_feature': batch_code_feature,\n",
    "                'util_feature': batch_util_feature,\n",
    "                'date_feature': batch_date_feature,\n",
    "            },\n",
    "            {\n",
    "                'cls_label': batch_cls\n",
    "            })\n",
    "        return dic\n",
    "    \n",
    "    def date_padding(self, seq):\n",
    "        seq = [x[:-1] for x in seq]\n",
    "        \n",
    "        pad_seq = np.zeros((len(seq), self.max_visit))\n",
    "        for i, p in enumerate(seq):\n",
    "            pad_seq[i][:len(p)] = p[:self.max_visit]\n",
    "        return pad_seq\n",
    "    \n",
    "    def code_padding(self, seq):\n",
    "        seq = [x[:-1] for x in seq]\n",
    "        \n",
    "        X = np.zeros((len(seq), self.max_visit, self.max_code))\n",
    "        for i, p in enumerate(seq):\n",
    "            if len(p) > self.max_visit: \n",
    "                p = p[:self.max_visit]\n",
    "            for j, claim in enumerate(p):\n",
    "                claim = claim[:self.max_code]\n",
    "                X[i][j][:len(claim)] = claim\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "quick-webster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 18.3 ms\n"
     ]
    }
   ],
   "source": [
    "def create_code_mask(code_seq):\n",
    "    code_mask = tf.cast(tf.math.not_equal(code_seq, 0), tf.float32)\n",
    "    return code_mask[:,:,:,tf.newaxis]\n",
    "\n",
    "def create_visit_mask(seq):\n",
    "    visit_mask = tf.cast(tf.math.not_equal(seq, 0), tf.float32)\n",
    "    return visit_mask[:,:]\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V, Q_masks, K_masks):\n",
    "    d_k = K.get_shape().as_list()[-1] # d_model/h\n",
    "\n",
    "    outputs = tf.matmul(Q, tf.transpose(K, [0, 2, 1]))  # (h*N, T_q, T_k)\n",
    "    outputs /= d_k ** 0.5\n",
    "\n",
    "    padding_num = -1e+7\n",
    "    K_masks = tf.expand_dims(K_masks, 1) # (h*N, 1, T_k)\n",
    "    K_masks = tf.tile(K_masks, [1, tf.shape(Q)[1], 1]) # (h*N, T_q, T_k)\n",
    "    paddings = tf.ones_like(outputs) * padding_num\n",
    "    outputs = tf.where(tf.equal(K_masks, 0), paddings, outputs)  # (h*N, T_q, T_k)\n",
    "\n",
    "    outputs = tf.nn.softmax(outputs)\n",
    "    attention = outputs\n",
    "    Q_masks = tf.expand_dims(Q_masks, -1) # (h*N, T_q, 1)\n",
    "    Q_masks = tf.tile(Q_masks, [1, 1, tf.shape(K)[1]]) # (h*N, T_q, T_k)\n",
    "    outputs = outputs * tf.cast(Q_masks, dtype=tf.float32)\n",
    "\n",
    "    return tf.matmul(outputs, V), attention # [h*N, T_q, d_model/h]\n",
    "\n",
    "class multihead_attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, name=\"multihead_attention\"):\n",
    "        super(multihead_attention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.query_dense = layers.Dense(units=d_model, use_bias=False)\n",
    "        self.key_dense = layers.Dense(units=d_model, use_bias=False)\n",
    "        self.value_dense = layers.Dense(units=d_model, use_bias=False)\n",
    "        self.add =layers.Add()\n",
    "        self.norm = layers.LayerNormalization()\n",
    "    \n",
    "    def call(self, queries, keys, values, query_masks, key_masks):\n",
    "        Q = self.query_dense(queries)\n",
    "        K = self.key_dense(keys)\n",
    "        V = self.value_dense(values)\n",
    "\n",
    "        # Split and concat\n",
    "        Q_ = tf.concat(tf.split(Q, self.num_heads, axis=2), axis=0) # (h*N, T_q, d_model/h)\n",
    "        K_ = tf.concat(tf.split(K, self.num_heads, axis=2), axis=0) # (h*N, T_k, d_model/h)\n",
    "        V_ = tf.concat(tf.split(V, self.num_heads, axis=2), axis=0) # (h*N, T_v, d_model/h)\n",
    "        query_masks = tf.tile(query_masks, [self.num_heads, 1]) # (h*N, T_q)\n",
    "        key_masks = tf.tile(key_masks, [self.num_heads, 1]) # (h*N, T_k)\n",
    "\n",
    "        # Attention\n",
    "        outputs, attention = scaled_dot_product_attention(Q_, K_, V_, query_masks, key_masks) # (h*N, T_q, d_model/h)\n",
    "\n",
    "        # Restore shape\n",
    "        outputs = tf.concat(tf.split(outputs, self.num_heads, axis=0), axis=2) # (N, T_q, d_model)\n",
    "\n",
    "        # Residual connection\n",
    "        outputs = self.add([queries, outputs])\n",
    "        outputs = self.norm(outputs)\n",
    "        \n",
    "        return outputs, attention\n",
    "\n",
    "class ffn(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, ffn_dim, name=\"ffn\"):\n",
    "        super(ffn, self).__init__(name=name)\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.dense1 = layers.Dense(units=ffn_dim, activation=tf.nn.relu, use_bias=False)\n",
    "        self.dense2 = layers.Dense(units=d_model, use_bias=False)\n",
    "        self.add =layers.Add()\n",
    "        self.norm = layers.LayerNormalization()\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        outputs = self.dense1(inputs)\n",
    "        outputs = self.dense2(outputs)\n",
    "        outputs = self.add([inputs, outputs])\n",
    "        outputs = self.norm(outputs)\n",
    "        return outputs\n",
    "\n",
    "def cat_recall(y_true, y_pred):\n",
    "    mask_value = tf.cast(tf.not_equal(tf.reduce_sum(y_true,axis=-1), 0), tf.float32)\n",
    "    true_positives = tf.cast(tf.reduce_sum(tf.multiply(tf.round(y_pred), y_true), axis=-1), tf.float32)\n",
    "    possible_positives = tf.cast(tf.reduce_sum(y_true, axis=-1), tf.float32)\n",
    "    values = true_positives / (possible_positives + 1e-7)\n",
    "    return tf.reduce_sum(values)/tf.reduce_sum(mask_value)\n",
    "\n",
    "def cat_loss_fun(y_true, y_pred):\n",
    "    loss = tf.cast(tf.keras.losses.BinaryCrossentropy(reduction='none')(y_true, y_pred), tf.float32)\n",
    "    mask = tf.cast(tf.not_equal(tf.reduce_sum(y_true,axis=-1), 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "    # return tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "    return loss\n",
    "\n",
    "def model(\n",
    "    max_visit,\n",
    "    max_code,\n",
    "    max_demo,\n",
    "    \n",
    "    demo_vocab,\n",
    "    code_vocab,\n",
    "    date_vocab,\n",
    "    util_vocab,\n",
    "    cat_vocab,\n",
    "\n",
    "    patient_dim,\n",
    "    vocab_dim=100,\n",
    "    model_dim=100,\n",
    "    ffn_dim=100,\n",
    "    num_heads=2,\n",
    "    num_translayer=1,\n",
    "    \n",
    "    model_name=\"TransF\"):\n",
    "    \n",
    "    demo = layers.Input(shape=(max_demo, ), name=\"demo_feature\")  # max_demo = 2, age&sex\n",
    "    code_seq = layers.Input(shape=(max_visit, max_code), name=\"code_feature\") \n",
    "    util_seq = layers.Input(shape=(max_visit), name=\"util_feature\")\n",
    "    date_seq = layers.Input(shape=(max_visit), name=\"date_feature\")\n",
    "\n",
    "    inputs = [demo, code_seq, util_seq, date_seq]\n",
    "    \n",
    "    # demo embedding\n",
    "    demo_emb = layers.Embedding(input_dim=demo_vocab, output_dim=vocab_dim, mask_zero=True, name='demo_embedding')(demo)\n",
    "    demo_emb = layers.Lambda(lambda x: tf.keras.backend.sum(x, axis=1))(demo_emb)     \n",
    "\n",
    "    # code sequence\n",
    "    code_mask = layers.Lambda(create_code_mask)(code_seq)\n",
    "    code_emb = layers.Embedding(input_dim=code_vocab, \n",
    "                                output_dim=vocab_dim, \n",
    "                                name='code_embed')(code_seq)\n",
    "    code_emb = layers.Multiply()([code_emb, code_mask])\n",
    "    code_emb = tf.reduce_sum(code_emb, axis=2)     \n",
    "\n",
    "    \n",
    "    # visit mask\n",
    "    visit_mask = layers.Lambda(create_visit_mask)(date_seq)\n",
    "    \n",
    "    # util sequence   \n",
    "    util_emb = layers.Embedding(input_dim=util_vocab, output_dim=vocab_dim, mask_zero=True, name='util_embedding')(util_seq)\n",
    "    util_emb = layers.Multiply()([util_emb, visit_mask[:,:,tf.newaxis]])\n",
    "    \n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs, name=model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dependent-movie",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 27 ms\n"
     ]
    }
   ],
   "source": [
    "def process_code(seq, vocab2int):\n",
    "    unseen = []\n",
    "    new_seq = []\n",
    "    for p in seq:\n",
    "        new_p = []\n",
    "        for v in p:\n",
    "            new_v = []\n",
    "            for c in v:\n",
    "                if c not in vocab2int: \n",
    "                    unseen.append(c)\n",
    "                    continue\n",
    "                    # vocab2int[c] = len(vocab2int)\n",
    "                new_v.append(vocab2int[c])\n",
    "            new_p.append(new_v)\n",
    "        new_seq.append(new_p)\n",
    "        \n",
    "    print(\"UNSEEN VOCAB:\",len(set(unseen)), len(unseen))\n",
    "    return new_seq\n",
    "\n",
    "def process_util(seq, util2int):\n",
    "    new_seq = []\n",
    "    vocab2int = {\"PAD\":0,\"IP\":1,\"RX\":2,\"OP\":3}\n",
    "    for p in seq:\n",
    "        new_p = []\n",
    "        for v in p:\n",
    "            if \"IP\" in v:\n",
    "                new_v=1\n",
    "            elif \"RX\" in v:\n",
    "                new_v=2\n",
    "            else:\n",
    "                new_v=3\n",
    "            new_p.append(new_v)\n",
    "        new_seq.append(new_p)\n",
    "    return new_seq\n",
    "    \n",
    "def process_demo(age_seq, sex_seq, vocab2int):\n",
    "    new_seq = []\n",
    "    for age, sex in zip(age_seq, sex_seq):\n",
    "        p = []\n",
    "        assert age in vocab2int\n",
    "        assert sex in vocab2int\n",
    "        \n",
    "        p.append(vocab2int[age])\n",
    "        p.append(vocab2int[sex])\n",
    "        new_seq.append(p)\n",
    "    return np.array(new_seq)\n",
    "\n",
    "def get_cat(seq,code2cat):\n",
    "    new_seq = []\n",
    "    for p in seq:\n",
    "        new_p = []\n",
    "        for v in p:\n",
    "            new_v = []\n",
    "            for c in v:\n",
    "                new_c = code2cat[c]\n",
    "                new_v.append(new_c)\n",
    "            new_p.append(new_v)\n",
    "        new_seq.append(new_p)\n",
    "    return new_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ongoing-manufacturer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------LOADING DIC------\n",
      "time: 89.1 ms\n"
     ]
    }
   ],
   "source": [
    "print(\"------LOADING DIC------\")\n",
    "path = \"/Users/xxz005/Desktop/RAW_DATA/code2cat/\"\n",
    "\n",
    "diag2cat = pickle.load(open(path+\"diag2cat\",\"rb\"))\n",
    "proc2cat = pickle.load(open(path+\"proc2cat\",\"rb\"))\n",
    "drug2cat = pickle.load(open(path+\"drug2cat\",\"rb\"))\n",
    "\n",
    "code2cat = {**diag2cat, **proc2cat, **drug2cat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "wrong-warrior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 46.1 ms\n"
     ]
    }
   ],
   "source": [
    "code2int, util2int, demo2int, cat2int  = pickle.load(open(\"../../pretraining/model/vocabs/vocabs\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "invalid-silicon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNSEEN VOCAB: 0 0\n",
      "time: 11.3 ms\n"
     ]
    }
   ],
   "source": [
    "code_feature = process_code(code_seq, code2int)\n",
    "util_feature = process_util(util_seq, util2int)\n",
    "demo_feature = process_demo(age_seq, sex_seq, demo2int)\n",
    "date_feature = date_seq\n",
    "\n",
    "cls_feature = np.array(label_seq).reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adverse-arkansas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.21 ms\n"
     ]
    }
   ],
   "source": [
    "MAX_VISIT=30\n",
    "MAX_CODE=10\n",
    "MAX_DEMO=2\n",
    "PATIENT_DIM=100\n",
    "\n",
    "BATCH_SIZE = 500\n",
    "TRAIN_RATIO = 0.7\n",
    "DATA_SIZE = len(age_seq)\n",
    "EPOCHS = 20\n",
    "\n",
    "params = {\n",
    "    'seqs':[demo_feature, code_feature, util_feature, date_feature, cls_feature],\n",
    "    'vocab_sizes': [len(code2int), len(cat2int)],\n",
    "    'batch_size':100,\n",
    "    'max_visit':MAX_VISIT, \n",
    "    'max_code':MAX_CODE,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sorted-wholesale",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.69 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_IDs, valid_IDs = train_test_split(range(DATA_SIZE), train_size=TRAIN_RATIO, random_state=42)\n",
    "train_generator = DataGenerator(list_IDs=train_IDs, shuffle=True, **params)\n",
    "valid_generator = DataGenerator(list_IDs=valid_IDs, shuffle=False, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-layout",
   "metadata": {},
   "source": [
    "# Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "suspended-settlement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 417 µs\n"
     ]
    }
   ],
   "source": [
    "# model_path = \"../../pretraining/model/saveModel\"\n",
    "\n",
    "# model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "handmade-memorabilia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.67 s\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/Users/xxz005/Desktop/saveModel\"\n",
    "\n",
    "model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "involved-genetics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.8 ms\n"
     ]
    }
   ],
   "source": [
    "model_losses = {\n",
    "    \"cls_label\":tf.keras.losses.BinaryCrossentropy(),\n",
    "}\n",
    "\n",
    "model_metrics = {\n",
    "    \"cls_label\": tf.keras.metrics.AUC(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "veterinary-chinese",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"TransF\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "demo_feature (InputLayer)       [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "demo_embedding (Embedding)      (None, 2, 100)       2400        demo_feature[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 100)          0           demo_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "code_feature (InputLayer)       [(None, 30, 10)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "date_feature (InputLayer)       [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims (TensorF (None, 1, 100)       0           lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "code_embed (Embedding)          (None, 30, 10, 100)  4983600     code_feature[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 30, 10, 1)    0           code_feature[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 30)           0           date_feature[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "util_feature (InputLayer)       [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 30, 10, 100)  0           code_embed[0][0]                 \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "date_embedding (Embedding)      (None, 30, 100)      36600       date_feature[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te (None, 30, 1)        0           lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "util_embedding (Embedding)      (None, 30, 100)      400         util_feature[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens (None, 30, 1)        0           lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_1 (TensorFlowOp (None, 1)            0           tf_op_layer_ExpandDims[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum (TensorFlowOpLa (None, 30, 100)      0           multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 30, 100)      0           date_embedding[0][0]             \n",
      "                                                                 tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 30, 100)      0           util_embedding[0][0]             \n",
      "                                                                 tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp (2,)                 0           tf_op_layer_Sum_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 30, 100)      0           tf_op_layer_Sum[0][0]            \n",
      "                                                                 multiply_2[0][0]                 \n",
      "                                                                 multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Fill (TensorFlowOpL (None, 1)            0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat (TensorFlowO (None, 31, 100)      0           tf_op_layer_ExpandDims[0][0]     \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_4 (TensorFlo (None, 31)           0           tf_op_layer_Fill[0][0]           \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_1 (TensorFlo (None, 31, 100)      0           tf_op_layer_ExpandDims[0][0]     \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_3 (TensorFlo (None, 31)           0           tf_op_layer_Fill[0][0]           \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_2 (TensorFlo (None, 31, 100)      0           tf_op_layer_ExpandDims[0][0]     \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "multihead_attention-0 (multihea (None, 31, 100)      30200       tf_op_layer_concat[0][0]         \n",
      "                                                                 tf_op_layer_concat_4[0][0]       \n",
      "                                                                 tf_op_layer_concat_1[0][0]       \n",
      "                                                                 tf_op_layer_concat_3[0][0]       \n",
      "                                                                 tf_op_layer_concat_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "ffn-0 (ffn)                     (None, 31, 100)      20200       multihead_attention-0[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te (None, 1, 100)       0           ffn-0[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Squeeze (TensorFlow (None, 100)          0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "patient_embedding (Dense)       (None, 100)          10100       tf_op_layer_Squeeze[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te (None, 30, 100)      0           ffn-0[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 100)          10100       patient_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "code_label (Dense)              (None, 3357)         339057      patient_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cat_label (Dense)               (None, 30, 3357)     339057      tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "cls_label (Dense)               (None, 1)            101         dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,771,815\n",
      "Trainable params: 5,771,815\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "time: 34.3 ms\n"
     ]
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=opt, loss=model_losses, metrics=model_metrics)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "vanilla-karen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 25ms/step - loss: 5.3384 - cls_label_loss: 5.3384 - cls_label_auc: 0.5191\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.3545451164245605, 5.3545451164245605, 0.5202370285987854]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.48 s\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(valid_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "verbal-kingdom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['code_label/kernel:0', 'code_label/bias:0', 'cat_label/kernel:0', 'cat_label/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['code_label/kernel:0', 'code_label/bias:0', 'cat_label/kernel:0', 'cat_label/bias:0'] when minimizing the loss.\n",
      "4/4 - 2s - loss: 2.8303 - cls_label_loss: 2.8303 - cls_label_auc: 0.5014 - val_loss: 0.8291 - val_cls_label_loss: 0.8291 - val_cls_label_auc: 0.5882\n",
      "Epoch 2/20\n",
      "4/4 - 0s - loss: 1.3828 - cls_label_loss: 1.3828 - cls_label_auc: 0.4872 - val_loss: 1.3247 - val_cls_label_loss: 1.3247 - val_cls_label_auc: 0.5479\n",
      "Epoch 3/20\n",
      "4/4 - 0s - loss: 1.4830 - cls_label_loss: 1.4830 - cls_label_auc: 0.5295 - val_loss: 0.9591 - val_cls_label_loss: 0.9591 - val_cls_label_auc: 0.5396\n",
      "Epoch 4/20\n",
      "4/4 - 0s - loss: 0.9104 - cls_label_loss: 0.9104 - cls_label_auc: 0.5244 - val_loss: 0.8159 - val_cls_label_loss: 0.8159 - val_cls_label_auc: 0.5259\n",
      "Epoch 5/20\n",
      "4/4 - 0s - loss: 0.8908 - cls_label_loss: 0.8908 - cls_label_auc: 0.5669 - val_loss: 1.0094 - val_cls_label_loss: 1.0094 - val_cls_label_auc: 0.5249\n",
      "Epoch 6/20\n",
      "4/4 - 0s - loss: 0.7480 - cls_label_loss: 0.7480 - cls_label_auc: 0.6431 - val_loss: 0.6296 - val_cls_label_loss: 0.6296 - val_cls_label_auc: 0.5945\n",
      "Epoch 7/20\n",
      "4/4 - 0s - loss: 0.6394 - cls_label_loss: 0.6394 - cls_label_auc: 0.6756 - val_loss: 0.6210 - val_cls_label_loss: 0.6210 - val_cls_label_auc: 0.6494\n",
      "Epoch 8/20\n",
      "4/4 - 0s - loss: 0.6747 - cls_label_loss: 0.6747 - cls_label_auc: 0.6974 - val_loss: 0.5902 - val_cls_label_loss: 0.5902 - val_cls_label_auc: 0.6696\n",
      "Epoch 9/20\n",
      "4/4 - 0s - loss: 0.5923 - cls_label_loss: 0.5923 - cls_label_auc: 0.7106 - val_loss: 0.6152 - val_cls_label_loss: 0.6152 - val_cls_label_auc: 0.6576\n",
      "Epoch 10/20\n",
      "4/4 - 0s - loss: 0.5903 - cls_label_loss: 0.5903 - cls_label_auc: 0.7281 - val_loss: 0.6278 - val_cls_label_loss: 0.6278 - val_cls_label_auc: 0.6683\n",
      "Epoch 11/20\n",
      "4/4 - 0s - loss: 0.5430 - cls_label_loss: 0.5430 - cls_label_auc: 0.7677 - val_loss: 0.5627 - val_cls_label_loss: 0.5627 - val_cls_label_auc: 0.6906\n",
      "Epoch 12/20\n",
      "4/4 - 0s - loss: 0.5206 - cls_label_loss: 0.5206 - cls_label_auc: 0.7955 - val_loss: 0.5575 - val_cls_label_loss: 0.5575 - val_cls_label_auc: 0.6949\n",
      "Epoch 13/20\n",
      "4/4 - 0s - loss: 0.4970 - cls_label_loss: 0.4970 - cls_label_auc: 0.8233 - val_loss: 0.5646 - val_cls_label_loss: 0.5646 - val_cls_label_auc: 0.6961\n",
      "Epoch 14/20\n",
      "4/4 - 0s - loss: 0.4812 - cls_label_loss: 0.4812 - cls_label_auc: 0.8363 - val_loss: 0.5748 - val_cls_label_loss: 0.5748 - val_cls_label_auc: 0.7066\n",
      "Epoch 15/20\n",
      "4/4 - 0s - loss: 0.4676 - cls_label_loss: 0.4676 - cls_label_auc: 0.8548 - val_loss: 0.5503 - val_cls_label_loss: 0.5503 - val_cls_label_auc: 0.7186\n",
      "Epoch 16/20\n",
      "4/4 - 1s - loss: 0.4579 - cls_label_loss: 0.4579 - cls_label_auc: 0.8611 - val_loss: 0.5309 - val_cls_label_loss: 0.5309 - val_cls_label_auc: 0.7301\n",
      "Epoch 17/20\n",
      "4/4 - 0s - loss: 0.4415 - cls_label_loss: 0.4415 - cls_label_auc: 0.8793 - val_loss: 0.5320 - val_cls_label_loss: 0.5320 - val_cls_label_auc: 0.7366\n",
      "Epoch 18/20\n",
      "4/4 - 0s - loss: 0.4196 - cls_label_loss: 0.4196 - cls_label_auc: 0.8958 - val_loss: 0.5427 - val_cls_label_loss: 0.5427 - val_cls_label_auc: 0.7388\n",
      "Epoch 19/20\n",
      "4/4 - 0s - loss: 0.4135 - cls_label_loss: 0.4135 - cls_label_auc: 0.9036 - val_loss: 0.5512 - val_cls_label_loss: 0.5512 - val_cls_label_auc: 0.7338\n",
      "Epoch 20/20\n",
      "4/4 - 0s - loss: 0.3943 - cls_label_loss: 0.3943 - cls_label_auc: 0.9157 - val_loss: 0.5306 - val_cls_label_loss: 0.5306 - val_cls_label_auc: 0.7374\n",
      "time: 11.1 s\n"
     ]
    }
   ],
   "source": [
    "finetune_history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=valid_generator,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "conventional-makeup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 442 ms\n"
     ]
    }
   ],
   "source": [
    "outs = model.predict(valid_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "logical-opinion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((147, 3357), (147, 30, 3357), (147, 1))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.12 ms\n"
     ]
    }
   ],
   "source": [
    "outs[0].shape, outs[1].shape, outs[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-idaho",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-incentive",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "median-beads",
   "metadata": {},
   "source": [
    "# Cold Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "particular-corps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 512 ms\n"
     ]
    }
   ],
   "source": [
    "m = model(\n",
    "    patient_dim=PATIENT_DIM,\n",
    "    max_visit=MAX_VISIT,\n",
    "    max_code=MAX_CODE,\n",
    "    max_demo=MAX_DEMO,\n",
    "    code_vocab=len(code2int),\n",
    "    demo_vocab=len(demo2int),\n",
    "    util_vocab=4,\n",
    "    date_vocab=365,\n",
    "    cat_vocab=len(cat2int),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-congress",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "frozen-borough",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"TransF\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "demo_feature (InputLayer)       [(None, 2)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "demo_embedding (Embedding)      (None, 2, 100)       2400        demo_feature[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "code_feature (InputLayer)       [(None, 30, 10)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "date_feature (InputLayer)       [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 100)          0           demo_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "code_embed (Embedding)          (None, 30, 10, 100)  4983600     code_feature[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 30, 10, 1)    0           code_feature[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 30)           0           date_feature[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "util_feature (InputLayer)       [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_3 (TFOpLambda)   (None, 1, 100)       0           lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_9 (Multiply)           (None, 30, 10, 100)  0           code_embed[0][0]                 \n",
      "                                                                 lambda_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "date_embedding (Embedding)      (None, 30, 100)      36500       date_feature[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_13 (Sl (None, 30, 1)        0           lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "util_embedding (Embedding)      (None, 30, 100)      400         util_feature[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_12 (Sl (None, 30, 1)        0           lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum_6 (TFOpLambd (None, 30, 100)      0           multiply_9[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_11 (Multiply)          (None, 30, 100)      0           date_embedding[0][0]             \n",
      "                                                                 tf.__operators__.getitem_13[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "multiply_10 (Multiply)          (None, 30, 100)      0           util_embedding[0][0]             \n",
      "                                                                 tf.__operators__.getitem_12[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum_7 (TFOpLambd (None, 1)            0           tf.expand_dims_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 30, 100)      0           tf.math.reduce_sum_6[0][0]       \n",
      "                                                                 multiply_11[0][0]                \n",
      "                                                                 multiply_10[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.ones_like_3 (TFOpLambda)     (None, 1)            0           tf.math.reduce_sum_7[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_15 (TFOpLambda)       (None, 31, 100)      0           tf.expand_dims_3[0][0]           \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_19 (TFOpLambda)       (None, 31)           0           tf.ones_like_3[0][0]             \n",
      "                                                                 lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_16 (TFOpLambda)       (None, 31, 100)      0           tf.expand_dims_3[0][0]           \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_18 (TFOpLambda)       (None, 31)           0           tf.ones_like_3[0][0]             \n",
      "                                                                 lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_17 (TFOpLambda)       (None, 31, 100)      0           tf.expand_dims_3[0][0]           \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multihead_attention-0 (multihea (None, 31, 100)      30200       tf.concat_15[0][0]               \n",
      "                                                                 tf.concat_19[0][0]               \n",
      "                                                                 tf.concat_16[0][0]               \n",
      "                                                                 tf.concat_18[0][0]               \n",
      "                                                                 tf.concat_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "ffn-0 (ffn)                     (None, 31, 100)      20200       multihead_attention-0[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_14 (Sl (None, 1, 100)       0           ffn-0[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.squeeze_3 (TFOpLam (None, 100)          0           tf.__operators__.getitem_14[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "patient_embedding (Dense)       (None, 100)          10100       tf.compat.v1.squeeze_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "code_label (Dense)              (None, 3357)         339057      patient_embedding[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 5,422,457\n",
      "Trainable params: 5,422,457\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "time: 33.7 ms\n"
     ]
    }
   ],
   "source": [
    "m.compile(optimizer=\"RMSprop\", loss=tf.keras.losses.BinaryCrossentropy(), metrics=[tf.keras.metrics.Recall(top_k=5), \n",
    "                                                                                       tf.keras.metrics.Recall(top_k=10), \n",
    "                                                                                       tf.keras.metrics.Recall(top_k=30)])\n",
    "print(m.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "structural-space",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 325 µs\n"
     ]
    }
   ],
   "source": [
    "# opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "# m.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(), metrics=[tf.keras.metrics.Recall(top_k=5), \n",
    "#                                                                                        tf.keras.metrics.Recall(top_k=10), \n",
    "#                                                                                        tf.keras.metrics.Recall(top_k=30)])\n",
    "# print(m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "cleared-swimming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "70/70 - 11s - loss: 0.0524 - recall_41: 0.1668 - recall_42: 0.2301 - recall_43: 0.3743 - val_loss: 0.0047 - val_recall_41: 0.1927 - val_recall_42: 0.2697 - val_recall_43: 0.4610\n",
      "Epoch 2/20\n",
      "70/70 - 9s - loss: 0.0045 - recall_41: 0.1983 - recall_42: 0.2773 - recall_43: 0.4633 - val_loss: 0.0045 - val_recall_41: 0.1966 - val_recall_42: 0.2817 - val_recall_43: 0.4795\n",
      "Epoch 3/20\n",
      "70/70 - 9s - loss: 0.0044 - recall_41: 0.2138 - recall_42: 0.3049 - recall_43: 0.4900 - val_loss: 0.0044 - val_recall_41: 0.2262 - val_recall_42: 0.3118 - val_recall_43: 0.5048\n",
      "Epoch 4/20\n",
      "70/70 - 9s - loss: 0.0043 - recall_41: 0.2468 - recall_42: 0.3443 - recall_43: 0.5371 - val_loss: 0.0043 - val_recall_41: 0.2426 - val_recall_42: 0.3308 - val_recall_43: 0.5269\n",
      "Epoch 5/20\n",
      "70/70 - 9s - loss: 0.0041 - recall_41: 0.2795 - recall_42: 0.3847 - recall_43: 0.5798 - val_loss: 0.0043 - val_recall_41: 0.2471 - val_recall_42: 0.3388 - val_recall_43: 0.5346\n",
      "Epoch 6/20\n",
      "70/70 - 9s - loss: 0.0039 - recall_41: 0.3166 - recall_42: 0.4276 - recall_43: 0.6244 - val_loss: 0.0043 - val_recall_41: 0.2561 - val_recall_42: 0.3418 - val_recall_43: 0.5380\n",
      "Epoch 7/20\n",
      "70/70 - 9s - loss: 0.0037 - recall_41: 0.3552 - recall_42: 0.4698 - recall_43: 0.6668 - val_loss: 0.0044 - val_recall_41: 0.2526 - val_recall_42: 0.3348 - val_recall_43: 0.5261\n",
      "Epoch 8/20\n",
      "70/70 - 10s - loss: 0.0035 - recall_41: 0.3911 - recall_42: 0.5110 - recall_43: 0.7065 - val_loss: 0.0045 - val_recall_41: 0.2426 - val_recall_42: 0.3263 - val_recall_43: 0.5186\n",
      "Epoch 9/20\n",
      "70/70 - 10s - loss: 0.0033 - recall_41: 0.4259 - recall_42: 0.5546 - recall_43: 0.7375 - val_loss: 0.0046 - val_recall_41: 0.2367 - val_recall_42: 0.3212 - val_recall_43: 0.4980\n",
      "Epoch 10/20\n",
      "70/70 - 9s - loss: 0.0031 - recall_41: 0.4652 - recall_42: 0.5950 - recall_43: 0.7707 - val_loss: 0.0048 - val_recall_41: 0.2311 - val_recall_42: 0.3182 - val_recall_43: 0.5070\n",
      "Epoch 11/20\n",
      "70/70 - 9s - loss: 0.0029 - recall_41: 0.5005 - recall_42: 0.6295 - recall_43: 0.8009 - val_loss: 0.0049 - val_recall_41: 0.2246 - val_recall_42: 0.3073 - val_recall_43: 0.4877\n",
      "Epoch 12/20\n",
      "70/70 - 9s - loss: 0.0027 - recall_41: 0.5412 - recall_42: 0.6672 - recall_43: 0.8270 - val_loss: 0.0052 - val_recall_41: 0.2148 - val_recall_42: 0.3006 - val_recall_43: 0.4708\n",
      "Epoch 13/20\n",
      "70/70 - 9s - loss: 0.0025 - recall_41: 0.5758 - recall_42: 0.7015 - recall_43: 0.8495 - val_loss: 0.0054 - val_recall_41: 0.2174 - val_recall_42: 0.2999 - val_recall_43: 0.4682\n",
      "Epoch 14/20\n",
      "70/70 - 9s - loss: 0.0024 - recall_41: 0.6045 - recall_42: 0.7298 - recall_43: 0.8718 - val_loss: 0.0056 - val_recall_41: 0.1997 - val_recall_42: 0.2814 - val_recall_43: 0.4534\n",
      "Epoch 15/20\n",
      "70/70 - 9s - loss: 0.0022 - recall_41: 0.6374 - recall_42: 0.7570 - recall_43: 0.8873 - val_loss: 0.0058 - val_recall_41: 0.2096 - val_recall_42: 0.2888 - val_recall_43: 0.4469\n",
      "Epoch 16/20\n",
      "70/70 - 9s - loss: 0.0021 - recall_41: 0.6599 - recall_42: 0.7797 - recall_43: 0.9020 - val_loss: 0.0059 - val_recall_41: 0.2007 - val_recall_42: 0.2748 - val_recall_43: 0.4368\n",
      "Epoch 17/20\n",
      "70/70 - 9s - loss: 0.0019 - recall_41: 0.6871 - recall_42: 0.8025 - recall_43: 0.9161 - val_loss: 0.0063 - val_recall_41: 0.2057 - val_recall_42: 0.2788 - val_recall_43: 0.4330\n",
      "Epoch 18/20\n",
      "70/70 - 10s - loss: 0.0018 - recall_41: 0.7072 - recall_42: 0.8206 - recall_43: 0.9282 - val_loss: 0.0064 - val_recall_41: 0.1902 - val_recall_42: 0.2625 - val_recall_43: 0.4166\n",
      "Epoch 19/20\n",
      "70/70 - 9s - loss: 0.0017 - recall_41: 0.7264 - recall_42: 0.8364 - recall_43: 0.9356 - val_loss: 0.0066 - val_recall_41: 0.1876 - val_recall_42: 0.2670 - val_recall_43: 0.4220\n",
      "Epoch 20/20\n",
      "70/70 - 9s - loss: 0.0016 - recall_41: 0.7448 - recall_42: 0.8537 - recall_43: 0.9429 - val_loss: 0.0068 - val_recall_41: 0.1791 - val_recall_42: 0.2466 - val_recall_43: 0.4012\n",
      "time: 3min 7s\n"
     ]
    }
   ],
   "source": [
    "cold_start_his = m.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=valid_generator,\n",
    "    verbose=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-license",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-percentage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-variable",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
